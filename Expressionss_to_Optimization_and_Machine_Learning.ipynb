{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notpromising/Python-Assignments/blob/main/Expressionss_to_Optimization_and_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27QVWiRMqG5t"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQTuPnKyqG5w"
      },
      "source": [
        "NAME = \"Parth Mahale\"\n",
        "COLLABORATORS = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ew1z9MqG51"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dbbadf02bc7528ea3d64e08c2527d67c",
          "grade": false,
          "grade_id": "cell-5385e02c82c53cee",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0it9bFgGqG52"
      },
      "source": [
        "# Assignment 9: From Expressions to Optimization and Machine Learning\n",
        "\n",
        "_This notebook is a derivative work of [\"From Expressions to Optimization and Machine Learning\"](https://colab.research.google.com/drive/1br-B3hlv-C6mO9FkXSjV3hcULlIPvydE) by [Luca de Alfaro](https://sites.google.com/a/ucsc.edu/luca/home), which is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/), distributed by permission of the original author.  This notebook is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) by [Lindsey Kuper](https://users.soe.ucsc.edu/~lkuper/)._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5a43282d8534aa226326316f72c6f7b1",
          "grade": false,
          "grade_id": "cell-5b06e263ac0d930e",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "gQu9P1zLqG53"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "## Notebook Format\n",
        "\n",
        "This is a homework notebook.  It consists of various types of cells: \n",
        "\n",
        "* Text cells: you should read them!\n",
        "* Code cells: you should run them, as they may set up the problems that you are asked to solve.\n",
        "* **Solution** cells: These are cells where you should enter a solution.  You will see a marker in these cells that indicates where your work should be inserted:  \n",
        "\n",
        "```\n",
        "    # YOUR CODE HERE\n",
        "```    \n",
        "\n",
        "* Test cells: These cells contain some tests, and are worth some points.  You should run the cells as a way to debug your code, and to see if you understood the question, and whether the output of your code is produced in the correct format.\n",
        "\n",
        "**When we grade your notebook, we will run many additional tests in addition to the ones you see here.  (Otherwise, you'd be able to get full credit by hard-coding the desired output!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c1dd4fb8158a602bdd8e880140e25f23",
          "grade": false,
          "grade_id": "cell-c4cd419f461cda09",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "JqdnzMDQqG55"
      },
      "source": [
        "## Working on Your Notebook\n",
        "\n",
        "To work on your notebook, we recommend using Colab.  Working in Colab has many benefits:\n",
        "\n",
        "  * You don't have to maintain a working Python environment on your own machine; you can work from any machine that has an internet connection.\n",
        "  * Colab preserves the revision history, which is useful for many reasons.\n",
        "  * Your work is automatically saved in Google Drive.\n",
        "  \n",
        "In Colab, go to \"File > Save a copy in Drive...\" and then you'll have your own copy to work on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a589694ac4c9b7c80ac11631d7d4acc5",
          "grade": false,
          "grade_id": "cell-2360b9bd4f150e2d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xF6xVl0rqG57"
      },
      "source": [
        "## Submitting Your Notebook\n",
        "\n",
        "Before you turn your finished notebook in, it's a good idea to sure everything runs as expected. First, **factory reset the runtime** in Colab (go to \"Runtime > Factory reset runtime\").  Then, **run all the cells** (go to \"Runtime > Run all\") in Colab.\n",
        "\n",
        "(A word of caution: when you use \"Run all\", Colab tries to run cells in parallel, which can get you into trouble if later cells run before earlier ones on which they depend.  So, you might instead want to run all the cells sequentially yourself after resetting the runtime.  You can start at the top of the notebook, hold down the Shift key, and press Enter repeatedly to run one cell after another in order.)\n",
        "\n",
        "Submit your work as follows: \n",
        "\n",
        "  * Download the notebook from Colab, clicking on \"File > Download .ipynb\".\n",
        "  * Upload the resulting file to [this Google form](https://docs.google.com/forms/d/e/1FAIpQLScYTOpH2ZtlHFxWvLmj2AepJBKwTEOY-nLC8rddHHJx08br2g/viewform?usp=sf_link).\n",
        "  * **Deadline: 9am Friday, February 28.**\n",
        "\n",
        "You can submit multiple times, and the last submission before the deadline will be used to assign you a grade.\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and the names of anyone you collaborated with in the below cells.  For collaborators, list anyone who (for example) discussed the general techniques involved with you, or pointed you to useful Python documentation -- collaboration of that kind is encouraged.  Remember that sharing code or using others' code is not allowed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEMWCjBLqG58"
      },
      "source": [
        "NAME = \"Parth Mahale\"\n",
        "COLLABORATORS = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "accadbdde7a42580ec4a53a0ca4f7dc6",
          "grade": false,
          "grade_id": "cell-c7df26aff022ad46",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WcAC1ZgmqG6A"
      },
      "source": [
        "## What Happens Next? \n",
        "\n",
        "After you submit your notebook, your instructor at some point will retreat to a secret hideout, put on some good music, and run some mysterious scripts.  These will generate two things: \n",
        "\n",
        "  * Your grade, which goes into a spreadsheet. \n",
        "  * Feedback, shared to you as a PDF file on Google Drive.  The PDF shows your work, your grade, the tests that passed and those that failed, and any comments left by the instructor and the TAs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d31cb6485de1dc5f69570e2d5e522cab",
          "grade": false,
          "grade_id": "cell-939a99101a208f69",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "86anwZm2qG6B"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Make sure to run the following cell to make sure that the Python testing framework, `nose`, is installed.  (It's required for the rest of the notebook to work.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d253a6068affa77a33ff2b2bd6632f1c",
          "grade": false,
          "grade_id": "cell-40563cbe5c8f8ad1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "MROXtVEgqG6E"
      },
      "source": [
        "try:\n",
        "    from nose.tools import assert_equal, assert_almost_equal\n",
        "    from nose.tools import assert_true, assert_false\n",
        "    from nose.tools import assert_not_equal, assert_greater_equal\n",
        "except:\n",
        "    !pip install nose\n",
        "    from nose.tools import assert_equal, assert_almost_equal\n",
        "    from nose.tools import assert_true, assert_false\n",
        "    from nose.tools import assert_not_equal, assert_greater_equal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc69bc73a362e375f791e9b67263e281",
          "grade": false,
          "grade_id": "cell-431f93269dced8f4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "y81y9BVyqG6H"
      },
      "source": [
        "## From Expressions to Optimization and Machine Learning\n",
        "\n",
        "In this assignment, we will use the machine learning framework we developed [during lecture](https://colab.research.google.com/drive/12gLy5eRVXxj0ur7p3ylnbc6o5izh_dye) to train some models and make some predictions.  The next several cells set up the code we'll be using.\n",
        "\n",
        "To start with, here is our `Expr` class and its subclasses for each expression type (variables (`V`), `Plus`, `Minus`, and so on).  There is nothing new here; this code is exactly as it was in assignment 8.  If you want to understand it better, though, you could always review the [\"Expressions as Classes\" lecture notebook](https://colab.research.google.com/drive/1YWWaTCloFfU4fROR1v9K6W4gNgW2o0vb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ebf562c03cbe071b98d807c4074cd54c",
          "grade": false,
          "grade_id": "cell-f5e9d23455f293ad",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VGV0hrOPqG6I"
      },
      "source": [
        "class Expr:\n",
        "    \"\"\"Abstract class representing expressions\"\"\"\n",
        "    \n",
        "    def __init__(self, *args):\n",
        "        self.children = list(args)\n",
        "        self.child_values = None\n",
        "\n",
        "    def eval(self, env=None):\n",
        "        \"\"\"Evaluates the value of the expression with respect to a given \n",
        "        environment.\"\"\"\n",
        "        # First, we evaluate the children.\n",
        "        # This is done here using a list comprehension,\n",
        "        # but we also could have written a for loop.\n",
        "        child_values = [c.eval(env=env) if isinstance(c, Expr) else c\n",
        "                        for c in self.children]\n",
        "\n",
        "        # Then, we evaluate the expression itself.\n",
        "        if any([isinstance(v, Expr) for v in child_values]):\n",
        "            # Symbolic result.\n",
        "            return self.__class__(*child_values)\n",
        "        else:\n",
        "            # Concrete result.\n",
        "            return self.op(*child_values)\n",
        "\n",
        "    def op(self, *args):\n",
        "        \"\"\"The op method computes the value of the expression, given the\n",
        "        numerical value of its subexpressions.  It is not implemented in \n",
        "        Expr, but rather, each subclass of Expr should provide its \n",
        "        implementation.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Represents the expression as the name of the class,\n",
        "        followed by all the children in parentheses.\"\"\"\n",
        "        return \"%s(%s)\" % (self.__class__.__name__,\n",
        "                        ', '.join(repr(c) for c in self.children))\n",
        "\n",
        "    # Expression constructors\n",
        "    def __add__(self, other):\n",
        "        return Plus(self, other)\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return Plus(self, other)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Minus(self, other)\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        return Minus(other, self)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return Multiply(other, self)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return Divide(self, other)\n",
        "\n",
        "    def __rtruediv__(self, other):\n",
        "        return Divide(other, self)\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        return Power(self, other)\n",
        "\n",
        "    def __rpow__(self, other):\n",
        "        return Power(other, self)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)\n",
        "    \n",
        "class V(Expr):\n",
        "    \"\"\"Variable.\"\"\"\n",
        "    \n",
        "    def __init__(self, *args):\n",
        "        \"\"\"Variables must be of type string.\"\"\"\n",
        "        assert len(args) == 1\n",
        "        assert isinstance(args[0], str)\n",
        "        super().__init__(*args)\n",
        "\n",
        "    def eval(self, env=None):\n",
        "        \"\"\"If the variable is in the environment, returns the\n",
        "        value of the variable; otherwise, returns the expression.\"\"\"\n",
        "        if env is not None and self.children[0] in env:\n",
        "            return env[self.children[0]]\n",
        "        else:\n",
        "            return self\n",
        "        \n",
        "class Plus(Expr):\n",
        "    def op(self, x, y):\n",
        "        return x + y\n",
        "\n",
        "class Minus(Expr):\n",
        "    def op(self, x, y):\n",
        "        return x - y\n",
        "\n",
        "class Multiply(Expr):\n",
        "    def op(self, x, y):\n",
        "        return x * y\n",
        "\n",
        "class Divide(Expr):\n",
        "    def op(self, x, y):\n",
        "        return x / y\n",
        "\n",
        "class Power(Expr):\n",
        "    def op(self, x, y):\n",
        "        return x ** y\n",
        "\n",
        "class Negative(Expr):\n",
        "    def op(self, x):\n",
        "        return -x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fc058412befb707c4ae152012ebabd35",
          "grade": false,
          "grade_id": "cell-a7050b3b68d4b46e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "x_wHh7tnqG6M"
      },
      "source": [
        "To make sure everything is in order, let's try constructing and evaluating some expressions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b447f6eb50f0ed8cdc02f0dbf09d63ff",
          "grade": false,
          "grade_id": "cell-97b12b149a79a230",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WQKm8uRiqG6N",
        "outputId": "895e982a-9edc-4acc-d724-5299f80eb299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = (V('x') + V('y')) * (2 + V('x'))\n",
        "# Evaluate the above expression in an environment where x is bound to 4 and y is bound to 3.\n",
        "e.eval({'x': 4, 'y': 3})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3fb9057d8ffe39e835462802f786b229",
          "grade": false,
          "grade_id": "cell-9b2d4eeecb8c6144",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "E4bbbG9WqG6V",
        "outputId": "3ab1edb9-1f91-4b16-9e44-f0b9dbcbef79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = Power(V('x'), 2) + V('y') -1\n",
        "# Evaluate the above expression in an environment where x is bound to 6 and y is bound to 7.\n",
        "e.eval({'x': 6, 'y': 7})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4307d241a22a8bc35f643f4346895ede",
          "grade": false,
          "grade_id": "cell-11fc18409c83f7cf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "26pNw0EzqG6Y"
      },
      "source": [
        "Next, here's the code for computing derivatives of various expression types.  There's nothing new here, either; we've seen it all before during lecture or in assignment 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dc35a175907c75200c08056974f65ecb",
          "grade": false,
          "grade_id": "cell-b66d96c53958162f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BzvCgtePqG6a"
      },
      "source": [
        "def expr_derivative(self, var):\n",
        "    \"\"\"Computes the derivative of the expression with respect to var.\"\"\"\n",
        "    partials = [(c.derivative(var) if isinstance(c, Expr) else 0)\n",
        "                for c in self.children]\n",
        "    return self.op_derivative(var, partials).eval()\n",
        "\n",
        "def expr_op_derivative(self, var, partials):\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "# Extends the existing Expr class with the newly defined methods.\n",
        "Expr.derivative = expr_derivative\n",
        "Expr.op_derivative = expr_op_derivative\n",
        "\n",
        "def variable_derivative(self, var):\n",
        "    return 1 if self.children[0] == var else 0\n",
        "\n",
        "# Extends the existing V class with the newly defined method.\n",
        "V.derivative = variable_derivative\n",
        "\n",
        "def plus_op_derivative(self, var, partials):\n",
        "    return Plus(partials[0], partials[1])\n",
        "\n",
        "# Extends the existing Plus class...you get the idea.\n",
        "Plus.op_derivative = plus_op_derivative\n",
        "\n",
        "def minus_op_derivative(self, var, partials):\n",
        "    \"\"\"Implements derivative for Minus expressions.\"\"\"    \n",
        "    return Minus(partials[0], partials[1])\n",
        "\n",
        "Minus.op_derivative = minus_op_derivative\n",
        "\n",
        "def negative_op_derivative(self, var, partials):\n",
        "    return Negative(partials[0])\n",
        "\n",
        "Negative.op_derivative = negative_op_derivative\n",
        "\n",
        "def multiply_op_derivative(self, var, partials):\n",
        "    return Plus(\n",
        "        Multiply(partials[0], self.children[1]),\n",
        "        Multiply(partials[1], self.children[0])\n",
        "    )\n",
        "\n",
        "Multiply.op_derivative = multiply_op_derivative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8bc7c839b9f028459c016ce4ff819c3b",
          "grade": false,
          "grade_id": "cell-50c9dce3ad6604da",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "smMk7Hz-qG6d"
      },
      "source": [
        "We will briefly make sure that this works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fc03c530d1ad2387e76659eee1ea354b",
          "grade": false,
          "grade_id": "cell-c76b64f518f9c4f3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SLNgsB_BqG6e",
        "outputId": "d81626f9-a9bb-4845-99c1-b6ebbb77f1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = 5 + V('y')\n",
        "e.derivative('y')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcSWlwAOqG6h",
        "outputId": "04a6cb40-dacd-4efa-dbca-d2abf00c024d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = 4 * V('x')\n",
        "e.derivative('x')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Plus(Multiply(0, V('x')), 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "83694b8a08a8124af4f5ba7253fffe08",
          "grade": false,
          "grade_id": "cell-7cff47ec0d08da90",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Qlj9Cuc-qG6k"
      },
      "source": [
        "Lastly, here are the `train_model` function and the `plot_points_and_model` function that we developed [during lecture](https://colab.research.google.com/drive/12gLy5eRVXxj0ur7p3ylnbc6o5izh_dye).  We will use these for training and visualizing models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f5dad63ae6f2fe6da10d01bedfa36013",
          "grade": false,
          "grade_id": "cell-3be036f444b9fb67",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "kIlP9VdIqG6m"
      },
      "source": [
        "def train_model(loss, training_examples, params, var_x, var_y, delta=0.0001, num_iterations=10000):\n",
        "    env = {param:0 for param in params}\n",
        "    for iteration_idx in range(num_iterations):        \n",
        "        gradient = {param:0 for param in params} # Set all gradients to 0\n",
        "        total_loss = 0.\n",
        "        for x_val, y_val in training_examples:\n",
        "            env[var_x] = x_val\n",
        "            env[var_y] = y_val\n",
        "            for param in params:\n",
        "                gradient[param] += loss.derivative(param).eval(env=env)\n",
        "            total_loss += loss.eval(env=env)\n",
        "        if (iteration_idx + 1) % 100 == 0:\n",
        "            print(\"Loss:\", total_loss)\n",
        "            print(\"Params: \", {param:env[param] for param in params})\n",
        "            pass\n",
        "        for param in params:\n",
        "            env[param] = env[param] - (delta * gradient[param])\n",
        "    return total_loss, {param:env[param] for param in params}\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)\n",
        "matplotlib_params = {'legend.fontsize': 'large',\n",
        "              'axes.labelsize': 'large',\n",
        "              'axes.titlesize':'large',\n",
        "              'xtick.labelsize':'large',\n",
        "              'ytick.labelsize':'large'}\n",
        "matplotlib.rcParams.update(matplotlib_params)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def plot_points_and_model(points, var_x, model, params, xlabel='x', ylabel='y'):\n",
        "    env = params.copy()\n",
        "    fig, ax = plt.subplots()\n",
        "    xs, ys = zip(*points)\n",
        "    ax.plot(xs, ys, 'r+')\n",
        "    x_min, x_max = np.min(xs), np.max(xs)\n",
        "    step = (x_max - x_min) / 100\n",
        "    x_list = list(np.arange(x_min, x_max + step, step))\n",
        "    y_list = []\n",
        "    for x in x_list:\n",
        "        env[var_x] = x\n",
        "        y_list.append(model.eval(env=env))\n",
        "    ax.plot(x_list, y_list)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a11a42f11bec9343aed62d79a2bb02a7",
          "grade": false,
          "grade_id": "cell-3adcc7ab8dfe2463",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "lHRzGiRFqG6p"
      },
      "source": [
        "### Problem 1: Setting up a (slightly) fancier model to predict rental prices\n",
        "\n",
        "During lecture, we considered the problem of predicting prices of Santa Cruz rental units.  `rental_prices` is a list of training examples, where the first element in each tuple is the number of bedrooms in the rental unit and the second element is the monthly rent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3415e7ec96ffa9a49b0fcd257648607f",
          "grade": false,
          "grade_id": "cell-3e9697859fe80ee1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-uJHJ5e7qG6q"
      },
      "source": [
        "rental_prices = [\n",
        "    (2, 2700),\n",
        "    (0, 2045),\n",
        "    (2, 2845),\n",
        "    (2, 2950),\n",
        "    (1, 2100),\n",
        "    (2, 3100),\n",
        "    (2, 2600),\n",
        "    (0, 2200),\n",
        "    (2, 2650),\n",
        "    (2, 2200),\n",
        "    (2, 2100),\n",
        "    (3, 3675),\n",
        "    (3, 3800),\n",
        "    (2, 4300),\n",
        "    (2, 2500),\n",
        "    (1, 2000),\n",
        "    (0, 1300),\n",
        "    (1, 2500),\n",
        "    (1, 1800),\n",
        "    (0, 1250),\n",
        "    (3, 3850),\n",
        "    (3, 4000)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e8e72da765d8f2882d99ef2856694a87",
          "grade": false,
          "grade_id": "cell-45fba5a9581afa74",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "s_JVaa-3qG6u"
      },
      "source": [
        "We trained a model that learns the best line through the data.  The resulting model predicted that a 4-bedroom rental would cost around \\$4300 a month.  But what if the best fit for the data isn't a line at all?\n",
        "\n",
        "For this problem, you will use the same training data to train a model shaped like a _parabolic curve_ instead of a line.  In other words, instead of training a model represented by the equation\n",
        "\n",
        "$$\n",
        "\\hat{y} = ax + b \\;,\n",
        "$$\n",
        "\n",
        "you will train a model represented by the equation\n",
        "\n",
        "$$\n",
        "\\hat{y} = ax^2 + bx + c \\;.\n",
        "$$\n",
        "\n",
        "So, this time, instead of having two parameters to learn ($a$ and $b$), you now have three parameters to learn ($a$, $b$, and $c$).  The framework that we've already developed is perfectly capable of handling this, and you can continue to use the squared error loss function, $(\\hat{y} - y)^2$, as the function to minimize during training.\n",
        "\n",
        "In the following cell, define the variables and expressions needed to set up the problem.  **If you're not sure what to do here, look at the code we used to set up the $\\hat{y} = ax + b$ problem during lecture.  It's in the section [\"Back to machine learning: setting up the problem\"](https://colab.research.google.com/drive/12gLy5eRVXxj0ur7p3ylnbc6o5izh_dye#scrollTo=Q2Ah2C5VWLsc) of the lecture notebook.  You need to do something analogous to what was done there, but now using the model $\\hat{y} = ax^2 + bx + c$.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1212bdfdbbaa42f830e52931fd7d90c1",
          "grade": false,
          "grade_id": "cell-16ccb3f936659ee8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Psx-6QSTqG6v"
      },
      "source": [
        "# In this cell, you need to define the following variables:\n",
        "#   * Variables `a`, `b`, and `c`, representing the parameters that we want to train.\n",
        "#   * Variables `x` and `y`, representing the pairs in the set of training examples.\n",
        "#   * A variable `y_hat` representing the model we will train.\n",
        "#   * A variable `loss` representing the loss function.\n",
        "\n",
        "# Variables `a`, `b`, and `c`, representing the parameters that we want to train\n",
        "a = V('a')\n",
        "b = V('b')\n",
        "c = V('c')\n",
        "\n",
        "\n",
        "# Variables `x` and `y`, representing the pairs in the set of training examples\n",
        "x = V('x')\n",
        "y = V('y')\n",
        "\n",
        "# A variable `y_hat` representing the model we will train.\n",
        "y_hat = (a * x*x) + b*(x)+c\n",
        "\n",
        "\n",
        "# A variable `loss` representing the loss function.\n",
        "loss = (y_hat - y) * (y_hat - y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ce59d8e297f8b23113e81b59a16f8761",
          "grade": true,
          "grade_id": "cell-d69ac7e6fd9d8fa2",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vY3zzAsrqG6y"
      },
      "source": [
        "# Tests for problem setup.\n",
        "# If you defined the above expressions correctly, the following tests should pass.\n",
        "assert_equal(y_hat.eval(env={'a': 0, 'b': 0, 'c': 0, 'x': 0, 'y': 0}), 0)\n",
        "assert_equal(y_hat.eval(env={'a': 8, 'b': 4, 'c': 2, 'x': 2, 'y': 2}), 42)\n",
        "assert_equal(loss.eval(env={'a': 0, 'b': 0, 'c': 0, 'x': 0, 'y': 0}), 0)\n",
        "assert_equal(loss.eval(env={'a': 0, 'b': 1, 'c': 0, 'x': 2, 'y': 2}), 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe447bce86acabfc540dfe7b58a2d280",
          "grade": true,
          "grade_id": "cell-c1605eaac45a668d",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sgZWY3FQqG62"
      },
      "source": [
        "# More tests for problem setup.\n",
        "# If you defined the above expressions correctly, the following tests should pass.\n",
        "assert_equal(loss.eval(env={'a': 4, 'b': 5, 'c': 17, 'x': 0, 'y': 17}), 0)\n",
        "assert_equal(loss.eval(env={'a': 0, 'b': 2, 'c': 3, 'x': 1, 'y': 5}), 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "aee17d3c42f8a57316581f5fd3cc81f2",
          "grade": false,
          "grade_id": "cell-1c22376ec1f071e8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "m2r68MYIqG65"
      },
      "source": [
        "### Problem 2: Training our fancier model\n",
        "\n",
        "For this problem, you will call the `train_model` function to train your model using the `rental_prices` training data, and assign the result of calling `train_model` to the variables `total_loss` and `params`.  The `train_model` function will perform gradient descent and learn the settings of the parameters that minimize the loss.\n",
        "\n",
        "For reference, here was how we called the `train_model` function during lecture when we trained the $\\hat{y} = ax + b$ model (see the [\"Training a model that minimizes the loss\"](https://colab.research.google.com/drive/12gLy5eRVXxj0ur7p3ylnbc6o5izh_dye#scrollTo=S1ZKJxpM6jDn&line=10&uniqifier=1) section of the lecture notebook):\n",
        "\n",
        "```\n",
        "total_loss, params = train_model(loss, rental_prices, ['a', 'b'], 'x', 'y')\n",
        "```\n",
        "\n",
        "You need to do the analogous thing, but for the model $\\hat{y} = ax^2 + bx + c$.\n",
        "\n",
        "**Important: pass the argument `num_iterations=30000` to `train_model`.  Otherwise, training will end too soon, before the loss has been reasonably minimized.**\n",
        "\n",
        "Do not pass a `delta` argument to `train_model`; the `train_model` function should just use the already-specified default value for `delta`.\n",
        "\n",
        "After you've set up the call to `train_model`, you can run the following cell!  **Warning: if you've set things up correctly, `train_model` will take a few minutes to run.  Now might be a good time to go get some coffee.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "79ed0ab996c98be1d909a8c6bbe43316",
          "grade": false,
          "grade_id": "cell-255f7897d8a90ce5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "oc-Z0fFeqG66",
        "outputId": "80d55c1d-fdd1-4781-a846-b13af7b0eee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# In this cell, you need to call the train_model function,\n",
        "# and assign the result to the variables total_loss and params.\n",
        "# Your code should look like:\n",
        "#\n",
        "# total_loss, params = train_model(...)\n",
        "#\n",
        "# where ... is replaced by the appropriate arguments.\n",
        "\n",
        "total_loss, params = train_model(loss, rental_prices, ['a', 'b','c'], 'x', 'y',num_iterations=30000)\n",
        "total_loss, params\n",
        "\n",
        "print(\"Total loss:\", total_loss, \"Params:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 20598043.360091895\n",
            "Params:  {'a': 372.0396234129675, 'b': 282.0338529584453, 'c': 326.5276202458154}\n",
            "Loss: 15249495.998575501\n",
            "Params:  {'a': 305.2166871276627, 'b': 362.1259509758519, 'c': 532.6014502886093}\n",
            "Loss: 11724974.77571642\n",
            "Params:  {'a': 251.87331483275295, 'b': 424.28615724191775, 'c': 701.256527245521}\n",
            "Loss: 9398991.305553805\n",
            "Params:  {'a': 209.43063476662076, 'b': 471.99899722500004, 'c': 839.5388876296952}\n",
            "Loss: 7860754.046884042\n",
            "Params:  {'a': 175.79764874022965, 'b': 508.084903293072, 'c': 953.1595592995848}\n",
            "Loss: 6840461.954652707\n",
            "Params:  {'a': 149.28061655665888, 'b': 534.8269964137831, 'c': 1046.7480129347123}\n",
            "Loss: 6160902.577095005\n",
            "Params:  {'a': 128.50769820353372, 'b': 554.0730559437345, 'c': 1124.0572234502051}\n",
            "Loss: 5705667.174005716\n",
            "Params:  {'a': 112.36788754216609, 'b': 567.3181496319694, 'c': 1188.1298449585765}\n",
            "Loss: 5398277.968906235\n",
            "Params:  {'a': 99.96152724511033, 'b': 575.7715915032027, 'c': 1241.4328736595012}\n",
            "Loss: 5188481.22873268\n",
            "Params:  {'a': 90.56020869657785, 'b': 580.4111997452139, 'c': 1285.9667745617257}\n",
            "Loss: 5043246.032617236\n",
            "Params:  {'a': 83.57427707397075, 'b': 582.0272630896367, 'c': 1323.3539146729956}\n",
            "Loss: 4940853.196755994\n",
            "Params:  {'a': 78.5264993468966, 'b': 581.2581674279193, 'c': 1354.910226938357}\n",
            "Loss: 4867013.460907627\n",
            "Params:  {'a': 75.03072644121735, 'b': 578.6192642743669, 'c': 1381.703285008464}\n",
            "Loss: 4812318.26413071\n",
            "Params:  {'a': 72.77460245824507, 'b': 574.5262627497215, 'c': 1404.59936585047}\n",
            "Loss: 4770565.616045826\n",
            "Params:  {'a': 71.50555344949386, 'b': 569.3141837006082, 'c': 1424.3015885108932}\n",
            "Loss: 4737660.634645414\n",
            "Params:  {'a': 71.01943379638887, 'b': 563.2527176056369, 'c': 1441.3808213142656}\n",
            "Loss: 4710893.46353213\n",
            "Params:  {'a': 71.15132619144521, 'b': 556.5586683067452, 'c': 1456.3007288579263}\n",
            "Loss: 4688465.013052424\n",
            "Params:  {'a': 71.76808679703771, 'b': 549.4060352610795, 'c': 1469.4380700997724}\n",
            "Loss: 4669175.448032557\n",
            "Params:  {'a': 72.76230461178996, 'b': 541.934182194092, 'c': 1481.09914809056}\n",
            "Loss: 4652219.5529728\n",
            "Params:  {'a': 74.0474068401403, 'b': 534.2544550969974, 'c': 1491.5331411228372}\n",
            "Loss: 4637052.286084713\n",
            "Params:  {'a': 75.55369292333617, 'b': 526.4555436817759, 'c': 1500.9429066755965}\n",
            "Loss: 4623300.429110627\n",
            "Params:  {'a': 77.22512110717183, 'b': 518.607824629915, 'c': 1509.493737385592}\n",
            "Loss: 4610704.511173483\n",
            "Params:  {'a': 79.01670482245736, 'b': 510.7668797716485, 'c': 1517.3204573958049}\n",
            "Loss: 4599080.616551942\n",
            "Params:  {'a': 80.89240322075706, 'b': 502.9763457046945, 'c': 1524.5331737856532}\n",
            "Loss: 4588295.253162177\n",
            "Params:  {'a': 82.82341214158652, 'b': 495.2702216798382, 'c': 1531.2219381077887}\n",
            "Loss: 4578248.80085603\n",
            "Params:  {'a': 84.7867795613908, 'b': 487.6747385279443, 'c': 1537.4605246942242}\n",
            "Loss: 4568864.596825277\n",
            "Params:  {'a': 86.76428397808138, 'b': 480.2098719115852, 'c': 1543.3094932037804}\n",
            "Loss: 4560081.725510451\n",
            "Params:  {'a': 88.74152585686711, 'b': 472.8905673894808, 'c': 1548.81867112422}\n",
            "Loss: 4551850.24374454\n",
            "Params:  {'a': 90.70719172160636, 'b': 465.7277319823495, 'c': 1554.0291662064485}\n",
            "Loss: 4544128.00747098\n",
            "Params:  {'a': 92.65245814067747, 'b': 458.7290365565058, 'c': 1558.9749979527608}\n",
            "Loss: 4536878.552441512\n",
            "Params:  {'a': 94.57050906757541, 'b': 451.8995649363231, 'c': 1563.6844203806852}\n",
            "Loss: 4530069.669163204\n",
            "Params:  {'a': 96.45614502975856, 'b': 445.2423388454475, 'c': 1568.180994588537}\n",
            "Loss: 4523672.4357388215\n",
            "Params:  {'a': 98.3054667380501, 'b': 438.75874225713375, 'c': 1572.4844585505189}\n",
            "Loss: 4517660.5532707125\n",
            "Params:  {'a': 100.11561899418192, 'b': 432.44886426131023, 'c': 1576.6114325755862}\n",
            "Loss: 4512009.88171374\n",
            "Params:  {'a': 101.8845834525121, 'b': 426.3117759314587, 'c': 1580.5759915762599}\n",
            "Loss: 4506698.10901525\n",
            "Params:  {'a': 103.6110109624668, 'b': 420.3457537372561, 'c': 1584.3901293875135}\n",
            "Loss: 4501704.509339039\n",
            "Params:  {'a': 105.29408597712688, 'b': 414.54845966881425, 'c': 1588.064135589923}\n",
            "Loss: 4497009.76125292\n",
            "Params:  {'a': 106.93341693870792, 'b': 408.9170863096348, 'c': 1591.6069014128395}\n",
            "Loss: 4492595.806669547\n",
            "Params:  {'a': 108.52894770669364, 'b': 403.44847353246155, 'c': 1595.0261681504485}\n",
            "Loss: 4488445.737843287\n",
            "Params:  {'a': 110.08088603035239, 'b': 398.1392022257049, 'c': 1598.3287289766622}\n",
            "Loss: 4484543.704007707\n",
            "Params:  {'a': 111.58964582583764, 'b': 392.9856694318028, 'c': 1601.520592980857}\n",
            "Loss: 4480874.832054782\n",
            "Params:  {'a': 113.05580063268752, 'b': 387.9841484472332, 'c': 1604.607118573963}\n",
            "Loss: 4477425.157510803\n",
            "Params:  {'a': 114.48004612260857, 'b': 383.13083675997535, 'c': 1607.5931220589805}\n",
            "Loss: 4474181.563285404\n",
            "Params:  {'a': 115.86316993702042, 'b': 378.4218941541256, 'c': 1610.4829660617108}\n",
            "Loss: 4471131.72447613\n",
            "Params:  {'a': 117.20602745689956, 'b': 373.8534728688674, 'c': 1613.2806316273577}\n",
            "Loss: 4468264.058043858\n",
            "Params:  {'a': 118.50952237348356, 'b': 369.4217413404214, 'c': 1615.989777067399}\n",
            "Loss: 4465567.676527637\n",
            "Params:  {'a': 119.77459114316063, 'b': 365.12290276505456, 'c': 1618.6137860565736}\n",
            "Loss: 4463032.345202804\n",
            "Params:  {'a': 121.00219058389533, 'b': 360.95320948579445, 'c': 1621.1558070061733}\n",
            "Loss: 4460648.442243646\n",
            "Params:  {'a': 122.193288011558, 'b': 356.9089740147347, 'c': 1623.6187853559534}\n",
            "Loss: 4458406.921558096\n",
            "Params:  {'a': 123.34885342880449, 'b': 352.9865773482413, 'c': 1626.0054901158749}\n",
            "Loss: 4456299.278034359\n",
            "Params:  {'a': 124.46985337173757, 'b': 349.18247510714093, 'c': 1628.3185357368266}\n",
            "Loss: 4454317.514989456\n",
            "Params:  {'a': 125.5572460946206, 'b': 345.4932019324971, 'c': 1630.5604001851307}\n",
            "Loss: 4452454.113644776\n",
            "Params:  {'a': 126.61197783370415, 'b': 341.91537448537, 'c': 1632.7334399301135}\n",
            "Loss: 4450702.004479171\n",
            "Params:  {'a': 127.63497994049129, 'b': 338.4456933323541, 'c': 1634.839902419802}\n",
            "Loss: 4449054.540328693\n",
            "Params:  {'a': 128.62716671468414, 'b': 335.08094394472994, 'c': 1636.881936511083}\n",
            "Loss: 4447505.471116468\n",
            "Params:  {'a': 129.5894337993904, 'b': 331.8179969953585, 'c': 1638.8616012325276}\n",
            "Loss: 4446048.920107214\n",
            "Params:  {'a': 130.52265702737606, 'b': 328.6538081020462, 'c': 1640.7808731866605}\n",
            "Loss: 4444679.361590089\n",
            "Params:  {'a': 131.4276916283803, 'b': 325.585417137423, 'c': 1642.6416528405412}\n",
            "Loss: 4443391.599901076\n",
            "Params:  {'a': 132.3053717247056, 'b': 322.6099472021651, 'c': 1644.4457699066386}\n",
            "Loss: 4442180.749702625\n",
            "Params:  {'a': 133.1565100562326, 'b': 319.724603339578, 'c': 1646.1949879779131}\n",
            "Loss: 4441042.217443946\n",
            "Params:  {'a': 133.98189788729513, 'b': 316.92667105433947, 'c': 1647.8910085502175}\n",
            "Loss: 4439971.683930475\n",
            "Params:  {'a': 134.78230505699358, 'b': 314.2135146858664, 'c': 1649.5354745401332}\n",
            "Loss: 4438965.087935623\n",
            "Params:  {'a': 135.55848014193336, 'b': 311.5825756768044, 'c': 1651.1299733860842}\n",
            "Loss: 4438018.610792096\n",
            "Params:  {'a': 136.311150706367, 'b': 309.03137076906137, 'c': 1652.6760398041788}\n",
            "Loss: 4437128.66190406\n",
            "Params:  {'a': 137.04102361958036, 'b': 306.557490153278, 'c': 1654.1751582568822}\n",
            "Loss: 4436291.8651249185\n",
            "Params:  {'a': 137.74878542429352, 'b': 304.15859559235287, 'c': 1655.6287651818498}\n",
            "Loss: 4435505.045948925\n",
            "Params:  {'a': 138.4351027430294, 'b': 301.8324185353682, 'c': 1657.0382510194625}\n",
            "Loss: 4434765.219467903\n",
            "Params:  {'a': 139.1006227119798, 'b': 299.5767582348209, 'c': 1658.4049620705534}\n",
            "Loss: 4434069.579047369\n",
            "Params:  {'a': 139.74597343398494, 'b': 297.3894798772727, 'c': 1659.730202209992}\n",
            "Loss: 4433415.485679026\n",
            "Params:  {'a': 140.37176444392708, 'b': 295.2685127352996, 'c': 1661.0152344771598}\n",
            "Loss: 4432800.457969234\n",
            "Params:  {'a': 140.9785871812009, 'b': 293.2118483467998, 'c': 1662.2612825605547}\n",
            "Loss: 4432222.162725427\n",
            "Params:  {'a': 141.56701546503183, 'b': 291.2175387262741, 'c': 1663.4695321906393}\n",
            "Loss: 4431678.406104785\n",
            "Params:  {'a': 142.13760596929444, 'b': 289.2836946115103, 'c': 1664.6411324526086}\n",
            "Loss: 4431167.125291575\n",
            "Params:  {'a': 142.69089869421205, 'b': 287.4084837481745, 'c': 1665.7771970286558}\n",
            "Loss: 4430686.3806715375\n",
            "Params:  {'a': 143.22741743288927, 'b': 285.5901292140496, 'c': 1666.8788053777184}\n",
            "Loss: 4430234.348473692\n",
            "Params:  {'a': 143.74767023110405, 'b': 283.82690778406766, 'c': 1667.9470038592988}\n",
            "Loss: 4429809.313851609\n",
            "Params:  {'a': 144.25214983915987, 'b': 282.1171483367939, 'c': 1668.9828068068753}\n",
            "Loss: 4429409.66437788\n",
            "Params:  {'a': 144.7413341548969, 'b': 280.4592303026549, 'c': 1669.987197555542}\n",
            "Loss: 4429033.88392717\n",
            "Params:  {'a': 145.21568665721273, 'b': 278.85158215388094, 'c': 1670.9611294277668}\n",
            "Loss: 4428680.54692457\n",
            "Params:  {'a': 145.67565682963107, 'b': 277.29267993591907, 'c': 1671.905526680559}\n",
            "Loss: 4428348.31293748\n",
            "Params:  {'a': 146.1216805736128, 'b': 275.781045839874, 'c': 1672.821285416893}\n",
            "Loss: 4428035.921590512\n",
            "Params:  {'a': 146.55418061143416, 'b': 274.31524681539855, 'c': 1673.7092744637735}\n",
            "Loss: 4427742.187784053\n",
            "Params:  {'a': 146.9735668785461, 'b': 272.8938932233431, 'c': 1674.5703362190611}\n",
            "Loss: 4427465.997198451\n",
            "Params:  {'a': 147.38023690541385, 'b': 271.515637527398, 'c': 1675.4052874688778}\n",
            "Loss: 4427206.3020666735\n",
            "Params:  {'a': 147.7745761888945, 'b': 270.17917302389685, 'c': 1676.214920177179}\n",
            "Loss: 4426962.117199467\n",
            "Params:  {'a': 148.15695855325882, 'b': 268.8832326089115, 'c': 1677.000002248929}\n",
            "Loss: 4426732.516247902\n",
            "Params:  {'a': 148.52774650099684, 'b': 267.6265875817398, 'c': 1677.7612782681463}\n",
            "Loss: 4426516.628189145\n",
            "Params:  {'a': 148.8872915535825, 'b': 266.40804648387103, 'c': 1678.499470211942}\n",
            "Loss: 4426313.634022104\n",
            "Params:  {'a': 149.23593458238372, 'b': 265.22645397250477, 'c': 1679.2152781416148}\n",
            "Loss: 4426122.763660448\n",
            "Params:  {'a': 149.57400612993192, 'b': 264.0806897276981, 'c': 1679.9093808717141}\n",
            "Loss: 4425943.293011169\n",
            "Params:  {'a': 149.9018267217635, 'b': 262.9696673922262, 'c': 1680.5824366179609}\n",
            "Loss: 4425774.541227637\n",
            "Params:  {'a': 150.21970716906273, 'b': 261.8923335432384, 'c': 1681.2350836248304}\n",
            "Loss: 4425615.868126718\n",
            "Params:  {'a': 150.52794886233605, 'b': 260.8476666948175, 'c': 1681.8679407735278}\n",
            "Loss: 4425466.671760147\n",
            "Params:  {'a': 150.82684405634706, 'b': 259.83467633055466, 'c': 1682.4816081710935}\n",
            "Loss: 4425326.386130959\n",
            "Params:  {'a': 151.11667614655084, 'b': 258.8524019652753, 'c': 1683.0766677212475}\n",
            "Loss: 4425194.4790462935\n",
            "Params:  {'a': 151.39771993725554, 'b': 257.8999122350656, 'c': 1683.653683677648}\n",
            "Loss: 4425070.450098454\n",
            "Params:  {'a': 151.67024190174163, 'b': 256.9763040147697, 'c': 1684.213203180111}\n",
            "Loss: 4424953.828766548\n",
            "Params:  {'a': 151.93450043456755, 'b': 256.0807015621505, 'c': 1684.7557567743854}\n",
            "Loss: 4424844.1726314835\n",
            "Params:  {'a': 152.1907460962822, 'b': 255.21225568791843, 'c': 1685.281858916007}\n",
            "Loss: 4424741.065697613\n",
            "Params:  {'a': 152.4392218507658, 'b': 254.37014295086433, 'c': 1685.7920084587395}\n",
            "Loss: 4424644.116814578\n",
            "Params:  {'a': 152.6801632954124, 'b': 253.55356487734545, 'c': 1686.2866891281049}\n",
            "Loss: 4424552.958193437\n",
            "Params:  {'a': 152.91379888436444, 'b': 252.7617472043955, 'c': 1686.7663699804634}\n",
            "Loss: 4424467.244011405\n",
            "Params:  {'a': 153.14035014500269, 'b': 251.993939145751, 'c': 1687.2315058481163}\n",
            "Loss: 4424386.6490999255\n",
            "Params:  {'a': 153.36003188789354, 'b': 251.24941268010537, 'c': 1687.6825377708406}\n",
            "Loss: 4424310.867711099\n",
            "Params:  {'a': 153.57305241038546, 'b': 250.52746186092318, 'c': 1688.1198934143097}\n",
            "Loss: 4424239.612357791\n",
            "Params:  {'a': 153.77961369404738, 'b': 249.82740214716225, 'c': 1688.5439874757724}\n",
            "Loss: 4424172.612723002\n",
            "Params:  {'a': 153.97991159612863, 'b': 249.14856975427662, 'c': 1688.9552220774242}\n",
            "Loss: 4424109.61463439\n",
            "Params:  {'a': 154.17413603522502, 'b': 248.49032102488587, 'c': 1689.3539871477988}\n",
            "Loss: 4424050.379100028\n",
            "Params:  {'a': 154.3624711713204, 'b': 247.8520318185168, 'c': 1689.7406607915996}\n",
            "Loss: 4423994.681401757\n",
            "Params:  {'a': 154.5450955803753, 'b': 247.23309691984315, 'c': 1690.1156096482862}\n",
            "Loss: 4423942.310242686\n",
            "Params:  {'a': 154.72218242362717, 'b': 246.63292946486055, 'c': 1690.4791892397816}\n",
            "Loss: 4423893.066945612\n",
            "Params:  {'a': 154.89389961175885, 'b': 246.05096038445743, 'c': 1690.8317443076282}\n",
            "Loss: 4423846.764699302\n",
            "Params:  {'a': 155.0604099640952, 'b': 245.48663786485255, 'c': 1691.1736091399007}\n",
            "Loss: 4423803.227849809\n",
            "Params:  {'a': 155.22187136297163, 'b': 244.93942682439118, 'c': 1691.5051078882157}\n",
            "Loss: 4423762.291234095\n",
            "Params:  {'a': 155.37843690342714, 'b': 244.4088084062025, 'c': 1691.8265548751067}\n",
            "Loss: 4423723.799553471\n",
            "Params:  {'a': 155.53025503835798, 'b': 243.89427948623802, 'c': 1692.1382548920851}\n",
            "Loss: 4423687.606784442\n",
            "Params:  {'a': 155.67746971927102, 'b': 243.39535219622675, 'c': 1692.4405034886615}\n",
            "Loss: 4423653.575624758\n",
            "Params:  {'a': 155.820220532772, 'b': 242.911553461093, 'c': 1692.7335872525882}\n",
            "Loss: 4423621.576972526\n",
            "Params:  {'a': 155.95864283291453, 'b': 242.44242455039947, 'c': 1693.0177840816154}\n",
            "Loss: 4423591.489436448\n",
            "Params:  {'a': 156.09286786953726, 'b': 241.98752064339112, 'c': 1693.2933634469964}\n",
            "Loss: 4423563.1988753155\n",
            "Params:  {'a': 156.22302291271032, 'b': 241.5464104072268, 'c': 1693.5605866490027}\n",
            "Loss: 4423536.597964984\n",
            "Params:  {'a': 156.3492313734081, 'b': 241.11867558799926, 'c': 1693.8197070646954}\n",
            "Loss: 4423511.5857912395\n",
            "Params:  {'a': 156.4716129205243, 'b': 240.70391061415572, 'c': 1694.0709703881726}\n",
            "Loss: 4423488.067466956\n",
            "Params:  {'a': 156.5902835943383, 'b': 240.3017222119431, 'c': 1694.3146148635421}\n",
            "Loss: 4423465.953772127\n",
            "Params:  {'a': 156.70535591654118, 'b': 239.91172903251382, 'c': 1694.5508715108238}\n",
            "Loss: 4423445.1608154075\n",
            "Params:  {'a': 156.81693899692692, 'b': 239.5335612903374, 'c': 1694.7799643449982}\n",
            "Loss: 4423425.609715855\n",
            "Params:  {'a': 156.92513863684727, 'b': 239.16686041257734, 'c': 1695.0021105884189}\n",
            "Loss: 4423407.226303675\n",
            "Params:  {'a': 157.03005742953042, 'b': 238.81127869909818, 'c': 1695.2175208767817}\n",
            "Loss: 4423389.940838863\n",
            "Params:  {'a': 157.13179485735756, 'b': 238.46647899278304, 'c': 1695.4263994588464}\n",
            "Loss: 4423373.687746632\n",
            "Params:  {'a': 157.23044738619012, 'b': 238.13213435984608, 'c': 1695.6289443901062}\n",
            "Loss: 4423358.40536863\n",
            "Params:  {'a': 157.3261085568378, 'b': 237.8079277798396, 'c': 1695.8253477205765}\n",
            "Loss: 4423344.035729056\n",
            "Params:  {'a': 157.4188690737511, 'b': 237.49355184506106, 'c': 1696.015795676898}\n",
            "Loss: 4423330.5243147025\n",
            "Params:  {'a': 157.50881689102607, 'b': 237.18870846907342, 'c': 1696.2004688389159}\n",
            "Loss: 4423317.819868156\n",
            "Params:  {'a': 157.5960372958009, 'b': 236.89310860406485, 'c': 1696.3795423109016}\n",
            "Loss: 4423305.874193352\n",
            "Params:  {'a': 157.68061298912374, 'b': 236.60647196677795, 'c': 1696.553185887583}\n",
            "Loss: 4423294.641972723\n",
            "Params:  {'a': 157.76262416436856, 'b': 236.32852677274983, 'c': 1696.7215642151439}\n",
            "Loss: 4423284.080595297\n",
            "Params:  {'a': 157.84214858327272, 'b': 236.05900947861016, 'c': 1696.8848369473392}\n",
            "Loss: 4423274.149995034\n",
            "Params:  {'a': 157.91926164966992, 'b': 235.797664532194, 'c': 1697.0431588968731}\n",
            "Loss: 4423264.81249883\n",
            "Params:  {'a': 157.99403648098644, 'b': 235.54424413023057, 'c': 1697.196680182197}\n",
            "Loss: 4423256.032683613\n",
            "Params:  {'a': 158.06654397757023, 'b': 235.29850798338046, 'c': 1697.3455463698424}\n",
            "Loss: 4423247.777241937\n",
            "Params:  {'a': 158.13685288991738, 'b': 235.0602230883967, 'c': 1697.4898986124483}\n",
            "Loss: 4423240.014855653\n",
            "Params:  {'a': 158.2050298838603, 'b': 234.82916350719543, 'c': 1697.6298737826003}\n",
            "Loss: 4423232.716077079\n",
            "Params:  {'a': 158.27113960377915, 'b': 234.605110152624, 'c': 1697.76560460261}\n",
            "Loss: 4423225.853217289\n",
            "Params:  {'a': 158.33524473389656, 'b': 234.38785058072642, 'c': 1697.8972197703627}\n",
            "Loss: 4423219.400241083\n",
            "Params:  {'a': 158.39740605771385, 'b': 234.1771787893073, 'c': 1698.0248440813446}\n",
            "Loss: 4423213.332668207\n",
            "Params:  {'a': 158.4576825156453, 'b': 233.9728950226035, 'c': 1698.1485985469699}\n",
            "Loss: 4423207.627480485\n",
            "Params:  {'a': 158.5161312609043, 'b': 233.77480558187906, 'c': 1698.2686005093276}\n",
            "Loss: 4423202.263034502\n",
            "Params:  {'a': 158.57280771369471, 'b': 233.58272264176256, 'c': 1698.3849637524409}\n",
            "Loss: 4423197.218979495\n",
            "Params:  {'a': 158.62776561376046, 'b': 233.3964640721546, 'c': 1698.4977986101524}\n",
            "Loss: 4423192.476180145\n",
            "Params:  {'a': 158.68105707133992, 'b': 233.21585326553486, 'c': 1698.6072120707447}\n",
            "Loss: 4423188.016643989\n",
            "Params:  {'a': 158.7327326165761, 'b': 233.04071896950498, 'c': 1698.7133078783838}\n",
            "Loss: 4423183.823453164\n",
            "Params:  {'a': 158.78284124742882, 'b': 232.87089512440983, 'c': 1698.81618663149}\n",
            "Loss: 4423179.880700223\n",
            "Params:  {'a': 158.83143047613396, 'b': 232.70622070588297, 'c': 1698.915945878118}\n",
            "Loss: 4423176.173427788\n",
            "Params:  {'a': 158.87854637425298, 'b': 232.54653957216465, 'c': 1699.0126802084594}\n",
            "Loss: 4423172.6875718115\n",
            "Params:  {'a': 158.92423361635844, 'b': 232.3917003160515, 'c': 1699.1064813445219}\n",
            "Loss: 4423169.409908218\n",
            "Params:  {'a': 158.96853552239295, 'b': 232.24155612133427, 'c': 1699.1974382271062}\n",
            "Loss: 4423166.32800274\n",
            "Params:  {'a': 159.0114940987449, 'b': 232.09596462358934, 'c': 1699.2856371001324}\n",
            "Loss: 4423163.430163744\n",
            "Params:  {'a': 159.05315007807772, 'b': 231.95478777519, 'c': 1699.37116159242}\n",
            "Loss: 4423160.705397879\n",
            "Params:  {'a': 159.0935429579524, 'b': 231.8178917144123, 'c': 1699.454092796984}\n",
            "Loss: 4423158.14336837\n",
            "Params:  {'a': 159.13271103827856, 'b': 231.68514663850996, 'c': 1699.5345093479207}\n",
            "Loss: 4423155.7343558045\n",
            "Params:  {'a': 159.17069145762974, 'b': 231.55642668063692, 'c': 1699.61248749497}\n",
            "Loss: 4423153.469221263\n",
            "Params:  {'a': 159.20752022845738, 'b': 231.43160979050174, 'c': 1699.688101175816}\n",
            "Loss: 4423151.339371646\n",
            "Params:  {'a': 159.2432322712371, 'b': 231.31057761864272, 'c': 1699.7614220861935}\n",
            "Loss: 4423149.336727073\n",
            "Params:  {'a': 159.2778614475811, 'b': 231.19321540420944, 'c': 1699.832519747863}\n",
            "Loss: 4423147.453690239\n",
            "Params:  {'a': 159.31144059234308, 'b': 231.07941186614914, 'c': 1699.9014615745402}\n",
            "Loss: 4423145.683117583\n",
            "Params:  {'a': 159.3440015447535, 'b': 230.9690590976915, 'c': 1699.9683129358075}\n",
            "Loss: 4423144.018292192\n",
            "Params:  {'a': 159.37557517860859, 'b': 230.8620524640329, 'c': 1700.0331372191004}\n",
            "Loss: 4423142.452898325\n",
            "Params:  {'a': 159.40619143154373, 'b': 230.7582905031235, 'c': 1700.0959958898113}\n",
            "Loss: 4423140.980997441\n",
            "Params:  {'a': 159.43587933342073, 'b': 230.65767482946248, 'c': 1700.1569485495684}\n",
            "Loss: 4423139.597005684\n",
            "Params:  {'a': 159.46466703385317, 'b': 230.56011004081043, 'c': 1700.2160529927485}\n",
            "Loss: 4423138.29567269\n",
            "Params:  {'a': 159.49258182889744, 'b': 230.46550362773013, 'c': 1700.273365261278}\n",
            "Loss: 4423137.072061673\n",
            "Params:  {'a': 159.51965018693403, 'b': 230.3737658858708, 'c': 1700.328939697774}\n",
            "Loss: 4423135.921530696\n",
            "Params:  {'a': 159.54589777376515, 'b': 230.28480983091163, 'c': 1700.3828289970647}\n",
            "Loss: 4423134.83971506\n",
            "Params:  {'a': 159.57134947695053, 'b': 230.19855111608496, 'c': 1700.4350842561566}\n",
            "Loss: 4423133.822510751\n",
            "Params:  {'a': 159.59602942940487, 'b': 230.11490795219956, 'c': 1700.4857550226864}\n",
            "Loss: 4423132.866058864\n",
            "Params:  {'a': 159.61996103228086, 'b': 230.03380103008953, 'c': 1700.5348893418973}\n",
            "Loss: 4423131.966730963\n",
            "Params:  {'a': 159.64316697715742, 'b': 229.9551534454146, 'c': 1700.582533802198}\n",
            "Loss: 4423131.121115329\n",
            "Params:  {'a': 159.66566926755598, 'b': 229.87889062574018, 'c': 1700.628733579335}\n",
            "Loss: 4423130.326003999\n",
            "Params:  {'a': 159.6874892398033, 'b': 229.80494025982983, 'c': 1700.6735324792298}\n",
            "Loss: 4423129.578380612\n",
            "Params:  {'a': 159.70864758326243, 'b': 229.7332322290801, 'c': 1700.7169729795125}\n",
            "Loss: 4423128.875408951\n",
            "Params:  {'a': 159.7291643599505, 'b': 229.66369854103525, 'c': 1700.7590962697993}\n",
            "Loss: 4423128.2144222\n",
            "Params:  {'a': 159.7490590235607, 'b': 229.59627326491812, 'c': 1700.7999422907487}\n",
            "Loss: 4423127.592912813\n",
            "Params:  {'a': 159.76835043790797, 'b': 229.53089246911543, 'c': 1700.8395497719316}\n",
            "Loss: 4423127.00852301\n",
            "Params:  {'a': 159.78705689481595, 'b': 229.46749416055872, 'c': 1700.8779562685538}\n",
            "Loss: 4423126.459035827\n",
            "Params:  {'a': 159.80519613146, 'b': 229.40601822594343, 'c': 1700.9151981970683}\n",
            "Loss: 4423125.942366708\n",
            "Params:  {'a': 159.82278534718668, 'b': 229.34640637473092, 'c': 1700.9513108696988}\n",
            "Loss: 4423125.4565555975\n",
            "Params:  {'a': 159.83984121982053, 'b': 229.28860208387744, 'c': 1700.9863285279293}\n",
            "Loss: 4423124.9997595055\n",
            "Params:  {'a': 159.8563799214785, 'b': 229.23255054424047, 'c': 1701.020284374966}\n",
            "Loss: 4423124.570245512\n",
            "Params:  {'a': 159.87241713390415, 'b': 229.17819860860936, 'c': 1701.0532106072226}\n",
            "Loss: 4423124.166384197\n",
            "Params:  {'a': 159.88796806333644, 'b': 229.1254947413119, 'c': 1701.0851384448472}\n",
            "Loss: 4423123.786643455\n",
            "Params:  {'a': 159.90304745492807, 'b': 229.07438896934895, 'c': 1701.1160981613275}\n",
            "Loss: 4423123.429582691\n",
            "Params:  {'a': 159.91766960672715, 'b': 229.0248328350106, 'c': 1701.1461191121928}\n",
            "Loss: 4423123.093847344\n",
            "Params:  {'a': 159.9318483832338, 'b': 228.9767793499301, 'c': 1701.1752297628473}\n",
            "Loss: 4423122.778163753\n",
            "Params:  {'a': 159.94559722854692, 'b': 228.93018295052954, 'c': 1701.2034577155623}\n",
            "Loss: 4423122.481334336\n",
            "Params:  {'a': 159.95892917911044, 'b': 228.88499945481686, 'c': 1701.2308297356528}\n",
            "Loss: 4423122.202233022\n",
            "Params:  {'a': 159.97185687607598, 'b': 228.84118602049426, 'c': 1701.2573717768464}\n",
            "Loss: 4423121.939801004\n",
            "Params:  {'a': 159.98439257728725, 'b': 228.7987011043347, 'c': 1701.2831090059015}\n",
            "Loss: 4423121.693042713\n",
            "Params:  {'a': 159.99654816890433, 'b': 228.75750442279204, 'c': 1701.308065826462}\n",
            "Loss: 4423121.461022033\n",
            "Params:  {'a': 160.00833517667257, 'b': 228.7175569138045, 'c': 1701.3322659022047}\n",
            "Loss: 4423121.242858765\n",
            "Params:  {'a': 160.0197647768523, 'b': 228.67882069975587, 'c': 1701.3557321792696}\n",
            "Loss: 4423121.037725275\n",
            "Params:  {'a': 160.03084780681507, 'b': 228.64125905156217, 'c': 1701.3784869080207}\n",
            "Loss: 4423120.8448433615\n",
            "Params:  {'a': 160.0415947753203, 'b': 228.60483635384432, 'c': 1701.4005516641416}\n",
            "Loss: 4423120.6634813\n",
            "Params:  {'a': 160.0520158724784, 'b': 228.56951807115865, 'c': 1701.4219473690982}\n",
            "Loss: 4423120.492951069\n",
            "Params:  {'a': 160.0621209794141, 'b': 228.5352707152507, 'c': 1701.44269430997}\n",
            "Loss: 4423120.332605738\n",
            "Params:  {'a': 160.0719196776349, 'b': 228.50206181330256, 'c': 1701.462812158687}\n",
            "Loss: 4423120.181837013\n",
            "Params:  {'a': 160.08142125811628, 'b': 228.46985987714243, 'c': 1701.4823199906862}\n",
            "Loss: 4423120.040072937\n",
            "Params:  {'a': 160.09063473011113, 'b': 228.43863437338806, 'c': 1701.5012363029944}\n",
            "Loss: 4423119.906775705\n",
            "Params:  {'a': 160.09956882969254, 'b': 228.40835569449544, 'c': 1701.5195790317684}\n",
            "Loss: 4423119.781439636\n",
            "Params:  {'a': 160.1082320280367, 'b': 228.37899513068515, 'c': 1701.5373655693033}\n",
            "Loss: 4423119.66358925\n",
            "Params:  {'a': 160.11663253945545, 'b': 228.35052484272092, 'c': 1701.5546127805212}\n",
            "Loss: 4423119.552777467\n",
            "Params:  {'a': 160.12477832918498, 'b': 228.3229178355118, 'c': 1701.5713370189637}\n",
            "Loss: 4423119.448583906\n",
            "Params:  {'a': 160.13267712093838, 'b': 228.29614793251687, 'c': 1701.587554142296}\n",
            "Loss: 4423119.350613293\n",
            "Params:  {'a': 160.14033640422866, 'b': 228.27018975092423, 'c': 1701.6032795273484}\n",
            "Loss: 4423119.258493966\n",
            "Params:  {'a': 160.1477634414711, 'b': 228.24501867758423, 'c': 1701.6185280846876}\n",
            "Loss: 4423119.171876456\n",
            "Params:  {'a': 160.15496527486854, 'b': 228.22061084567082, 'c': 1701.6333142727624}\n",
            "Loss: 4423119.09043217\n",
            "Params:  {'a': 160.16194873308902, 'b': 228.19694311205208, 'c': 1701.6476521116092}\n",
            "Loss: 4423119.0138521325\n",
            "Params:  {'a': 160.16872043774032, 'b': 228.1739930353456, 'c': 1701.6615551961486}\n",
            "Loss: 4423118.9418458305\n",
            "Params:  {'a': 160.17528680964924, 'b': 228.15173885464063, 'c': 1701.6750367090713}\n",
            "Loss: 4423118.874140096\n",
            "Params:  {'a': 160.1816540749489, 'b': 228.13015946886398, 'c': 1701.688109433341}\n",
            "Loss: 4423118.81047808\n",
            "Params:  {'a': 160.18782827098252, 'b': 228.10923441677275, 'c': 1701.7007857643162}\n",
            "Loss: 4423118.750618274\n",
            "Params:  {'a': 160.1938152520286, 'b': 228.08894385755335, 'c': 1701.7130777214963}\n",
            "Loss: 4423118.694333587\n",
            "Params:  {'a': 160.19962069485052, 'b': 228.06926855200868, 'c': 1701.7249969599288}\n",
            "Loss: 4423118.641410495\n",
            "Params:  {'a': 160.20525010408005, 'b': 228.05018984431553, 'c': 1701.7365547812528}\n",
            "Loss: 4423118.591648232\n",
            "Params:  {'a': 160.2107088174366, 'b': 228.03168964433561, 'c': 1701.747762144419}\n",
            "Loss: 4423118.544858014\n",
            "Params:  {'a': 160.21600201078888, 'b': 228.01375041046282, 'c': 1701.7586296760746}\n",
            "Loss: 4423118.500862335\n",
            "Params:  {'a': 160.22113470306175, 'b': 227.9963551329907, 'c': 1701.769167680647}\n",
            "Loss: 4423118.4594942955\n",
            "Params:  {'a': 160.22611176099497, 'b': 227.97948731798382, 'c': 1701.7793861501113}\n",
            "Loss: 4423118.420596958\n",
            "Params:  {'a': 160.23093790375884, 'b': 227.9631309716383, 'c': 1701.789294773464}\n",
            "Loss: 4423118.384022759\n",
            "Params:  {'a': 160.23561770742793, 'b': 227.94727058511768, 'c': 1701.7989029459084}\n",
            "Loss: 4423118.3496329505\n",
            "Params:  {'a': 160.24015560932014, 'b': 227.9318911198461, 'c': 1701.8082197777687}\n",
            "Loss: 4423118.317297069\n",
            "Params:  {'a': 160.2445559122047, 'b': 227.91697799325058, 'c': 1701.8172541031201}\n",
            "Loss: 4423118.286892446\n",
            "Params:  {'a': 160.24882278838152, 'b': 227.90251706493297, 'c': 1701.8260144881715}\n",
            "Loss: 4423118.2583037345\n",
            "Params:  {'a': 160.25296028363692, 'b': 227.88849462326294, 'c': 1701.8345092393847}\n",
            "Loss: 4423118.231422482\n",
            "Params:  {'a': 160.25697232108035, 'b': 227.87489737237593, 'c': 1701.8427464113568}\n",
            "Loss: 4423118.206146709\n",
            "Params:  {'a': 160.2608627048644, 'b': 227.8617124195671, 'c': 1701.8507338144448}\n",
            "Loss: 4423118.182380529\n",
            "Params:  {'a': 160.26463512379155, 'b': 227.84892726306643, 'c': 1701.8584790221817}\n",
            "Loss: 4423118.160033781\n",
            "Params:  {'a': 160.26829315481123, 'b': 227.83652978018452, 'c': 1701.8659893784554}\n",
            "Loss: 4423118.139021691\n",
            "Params:  {'a': 160.27184026641257, 'b': 227.8245082158181, 'c': 1701.873272004471}\n",
            "Loss: 4423118.119264546\n",
            "Params:  {'a': 160.27527982191245, 'b': 227.812851171304, 'c': 1701.8803338055022}\n",
            "Loss: 4423118.100687394\n",
            "Params:  {'a': 160.2786150826445, 'b': 227.80154759361125, 'c': 1701.8871814774416}\n",
            "Loss: 4423118.08321976\n",
            "Params:  {'a': 160.2818492110521, 'b': 227.79058676486085, 'c': 1701.8938215131454}\n",
            "Loss: 4423118.066795378\n",
            "Params:  {'a': 160.28498527368598, 'b': 227.77995829216272, 'c': 1701.900260208595}\n",
            "Loss: 4423118.051351942\n",
            "Params:  {'a': 160.2880262441129, 'b': 227.7696520977623, 'c': 1701.9065036688614}\n",
            "Loss: 4423118.036830863\n",
            "Params:  {'a': 160.2909750057343, 'b': 227.75965840948416, 'c': 1701.9125578138974}\n",
            "Loss: 4423118.023177054\n",
            "Params:  {'a': 160.2938343545207, 'b': 227.749967751466, 'c': 1701.9184283841525}\n",
            "Loss: 4423118.010338718\n",
            "Params:  {'a': 160.2966070016627, 'b': 227.74057093517442, 'c': 1701.9241209460108}\n",
            "Loss: 4423117.998267149\n",
            "Params:  {'a': 160.29929557614193, 'b': 227.73145905069217, 'c': 1701.9296408970724}\n",
            "Loss: 4423117.986916555\n",
            "Params:  {'a': 160.30190262722354, 'b': 227.72262345827028, 'c': 1701.9349934712695}\n",
            "Loss: 4423117.976243872\n",
            "Params:  {'a': 160.30443062687337, 'b': 227.71405578013517, 'c': 1701.9401837438313}\n",
            "Loss: 4423117.966208616\n",
            "Params:  {'a': 160.30688197210208, 'b': 227.70574789254601, 'c': 1701.9452166360943}\n",
            "Loss: 4423117.956772713\n",
            "Params:  {'a': 160.30925898723726, 'b': 227.69769191809092, 'c': 1701.950096920171}\n",
            "Loss: 4423117.947900369\n",
            "Params:  {'a': 160.31156392612863, 'b': 227.6898802182187, 'c': 1701.9548292234704}\n",
            "Loss: 4423117.939557924\n",
            "Params:  {'a': 160.3137989742838, 'b': 227.68230538599516, 'c': 1701.9594180330898}\n",
            "Loss: 4423117.931713731\n",
            "Params:  {'a': 160.31596625094107, 'b': 227.67496023908043, 'c': 1701.9638677000694}\n",
            "Loss: 4423117.924338032\n",
            "Params:  {'a': 160.31806781107903, 'b': 227.66783781291872, 'c': 1701.9681824435152}\n",
            "Loss: 4423117.917402845\n",
            "Params:  {'a': 160.32010564736478, 'b': 227.66093135413485, 'c': 1701.9723663546001}\n",
            "Loss: 4423117.910881862\n",
            "Params:  {'a': 160.32208169204347, 'b': 227.65423431413038, 'c': 1701.9764234004444}\n",
            "Loss: 4423117.904750343\n",
            "Params:  {'a': 160.32399781877018, 'b': 227.64774034287476, 'c': 1701.9803574278778}\n",
            "Loss: 4423117.898985028\n",
            "Params:  {'a': 160.3258558443871, 'b': 227.6414432828838, 'c': 1701.9841721670846}\n",
            "Loss: 4423117.893564049\n",
            "Params:  {'a': 160.3276575306455, 'b': 227.63533716338185, 'c': 1701.9878712351415}\n",
            "Loss: 4423117.888466836\n",
            "Params:  {'a': 160.32940458587694, 'b': 227.62941619463987, 'c': 1701.9914581394476}\n",
            "Loss: 4423117.883674053\n",
            "Params:  {'a': 160.33109866661226, 'b': 227.62367476248622, 'c': 1701.9949362810507}\n",
            "Loss: 4423117.8791675195\n",
            "Params:  {'a': 160.33274137915274, 'b': 227.61810742298286, 'c': 1701.9983089578734}\n",
            "Loss: 4423117.874930136\n",
            "Params:  {'a': 160.33433428109336, 'b': 227.61270889726393, 'c': 1702.0015793678333}\n",
            "Loss: 4423117.870945831\n",
            "Params:  {'a': 160.33587888279916, 'b': 227.60747406652968, 'c': 1702.004750611886}\n",
            "Loss: 4423117.867199489\n",
            "Params:  {'a': 160.33737664883802, 'b': 227.6023979671941, 'c': 1702.0078256969546}\n",
            "Loss: 4423117.863676895\n",
            "Params:  {'a': 160.33882899936864, 'b': 227.5974757861775, 'c': 1702.0108075387884}\n",
            "Loss: 4423117.8603646895\n",
            "Params:  {'a': 160.34023731148775, 'b': 227.59270285634358, 'c': 1702.0136989647215}\n",
            "Loss: 4423117.857250305\n",
            "Params:  {'a': 160.34160292053497, 'b': 227.58807465207335, 'c': 1702.0165027163628}\n",
            "Loss: 4423117.854321926\n",
            "Params:  {'a': 160.34292712135996, 'b': 227.58358678497427, 'c': 1702.019221452186}\n",
            "Loss: 4423117.851568444\n",
            "Params:  {'a': 160.3442111695498, 'b': 227.57923499971966, 'c': 1702.0218577500536}\n",
            "Loss: 4423117.848979413\n",
            "Params:  {'a': 160.3454562826191, 'b': 227.5750151700133, 'c': 1702.0244141096646}\n",
            "Loss: 4423117.846545012\n",
            "Params:  {'a': 160.34666364116518, 'b': 227.5709232946778, 'c': 1702.0268929549175}\n",
            "Loss: 4423117.844256006\n",
            "Params:  {'a': 160.34783438998664, 'b': 227.5669554938593, 'c': 1702.0292966362179}\n",
            "Loss: 4423117.84210371\n",
            "Params:  {'a': 160.34896963916992, 'b': 227.5631080053498, 'c': 1702.031627432698}\n",
            "Loss: 4423117.84007996\n",
            "Params:  {'a': 160.35007046514056, 'b': 227.55937718102, 'c': 1702.0338875543844}\n",
            "Loss: 4423117.838177078\n",
            "Params:  {'a': 160.35113791168507, 'b': 227.5557594833591, 'c': 1702.036079144289}\n",
            "Loss: 4423117.836387846\n",
            "Params:  {'a': 160.35217299093995, 'b': 227.55225148212193, 'c': 1702.0382042804436}\n",
            "Loss: 4423117.834705476\n",
            "Params:  {'a': 160.3531766843512, 'b': 227.54884985107532, 'c': 1702.0402649778712}\n",
            "Loss: 4423117.833123582\n",
            "Params:  {'a': 160.35414994360573, 'b': 227.5455513648448, 'c': 1702.0422631904921}\n",
            "Loss: 4423117.831636171\n",
            "Params:  {'a': 160.35509369153323, 'b': 227.54235289585654, 'c': 1702.0442008129799}\n",
            "Loss: 4423117.830237593\n",
            "Params:  {'a': 160.3560088229807, 'b': 227.53925141137103, 'c': 1702.0460796825605}\n",
            "Loss: 4423117.828922546\n",
            "Params:  {'a': 160.35689620566188, 'b': 227.5362439706085, 'c': 1702.0479015807484}\n",
            "Loss: 4423117.827686039\n",
            "Params:  {'a': 160.3577566809794, 'b': 227.53332772195927, 'c': 1702.0496682350406}\n",
            "Loss: 4423117.826523382\n",
            "Params:  {'a': 160.35859106482283, 'b': 227.53049990028143, 'c': 1702.05138132055}\n",
            "Loss: 4423117.825430166\n",
            "Params:  {'a': 160.35940014834216, 'b': 227.52775782427756, 'c': 1702.0530424615988}\n",
            "Loss: 4423117.824402242\n",
            "Params:  {'a': 160.36018469869785, 'b': 227.52509889395344, 'c': 1702.054653233256}\n",
            "Loss: 4423117.82343571\n",
            "Params:  {'a': 160.36094545978867, 'b': 227.52252058815202, 'c': 1702.0562151628294}\n",
            "Loss: 4423117.822526906\n",
            "Params:  {'a': 160.3616831529565, 'b': 227.52002046216322, 'c': 1702.0577297313173}\n",
            "Loss: 4423117.821672377\n",
            "Params:  {'a': 160.3623984776706, 'b': 227.51759614540615, 'c': 1702.0591983748106}\n",
            "Loss: 4423117.820868887\n",
            "Params:  {'a': 160.36309211219128, 'b': 227.51524533918075, 'c': 1702.0606224858532}\n",
            "Loss: 4423117.820113384\n",
            "Params:  {'a': 160.3637647142119, 'b': 227.51296581448844, 'c': 1702.0620034147662}\n",
            "Loss: 4423117.819403003\n",
            "Params:  {'a': 160.36441692148307, 'b': 227.51075540991877, 'c': 1702.063342470928}\n",
            "Loss: 4423117.81873505\n",
            "Params:  {'a': 160.36504935241817, 'b': 227.50861202960002, 'c': 1702.0646409240078}\n",
            "Loss: 4423117.81810699\n",
            "Params:  {'a': 160.3656626066785, 'b': 227.50653364121183, 'c': 1702.0659000051796}\n",
            "Loss: 4423117.817516441\n",
            "Params:  {'a': 160.36625726574212, 'b': 227.50451827405828, 'c': 1702.0671209082855}\n",
            "Loss: 4423117.816961164\n",
            "Params:  {'a': 160.3668338934561, 'b': 227.50256401719903, 'c': 1702.068304790965}\n",
            "Loss: 4423117.81643905\n",
            "Params:  {'a': 160.36739303656947, 'b': 227.50066901763776, 'c': 1702.0694527757603}\n",
            "Loss: 4423117.815948118\n",
            "Params:  {'a': 160.36793522525346, 'b': 227.49883147856502, 'c': 1702.0705659511711}\n",
            "Loss: 4423117.815486509\n",
            "Params:  {'a': 160.36846097360305, 'b': 227.49704965765434, 'c': 1702.0716453726936}\n",
            "Total loss: 4423117.815486509 Params: {'a': 160.36846614976236, 'b': 227.49703211506213, 'c': 1702.071655999941}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "951489d53bffa9669e72b330e29f5f8e",
          "grade": false,
          "grade_id": "cell-19a17873c5916b74",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "z2NTemN-qG6_"
      },
      "source": [
        "If you set up the call to `train_model` correctly, after it's finished running, the `total_loss` variable should be a number considerably less than `4941134.618681211`, which was the total loss for the $\\hat{y} = ax + b$ model that we trained during lecture.  The `params` variable should be a dictionary containing the trained values of the parameters to your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8f98076d4104995eb8f36e89aa3b4864",
          "grade": true,
          "grade_id": "cell-ace1933d9febaa8f",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "63iv855VqG7A"
      },
      "source": [
        "# Basic tests of the trained model.\n",
        "# If you called train_model with the correct arguments,\n",
        "# and assigned the result to the correct variables,\n",
        "# the following tests should pass.\n",
        "\n",
        "assert_greater_equal(4941134.618681211, total_loss)\n",
        "assert_equal(len(params), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "405f4ba93a1dc64e16fadd8dd3a7157c",
          "grade": true,
          "grade_id": "cell-94c425a1e06d2534",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "JAj_wGTMqG7D"
      },
      "source": [
        "# More specific tests of the trained model.\n",
        "# If you called train_model with the correct arguments,\n",
        "# and assigned the result to the correct variables,\n",
        "# the following tests should pass.\n",
        "\n",
        "# Note: If you're not ending up with these parameter values,\n",
        "# don't forget to use 30,000 training iterations.\n",
        "\n",
        "assert_greater_equal(params['a'], 160)\n",
        "assert_greater_equal(161, params['a'])\n",
        "\n",
        "assert_greater_equal(params['b'], 227)\n",
        "assert_greater_equal(228, params['b'])\n",
        "\n",
        "assert_greater_equal(params['c'], 1701.5)\n",
        "assert_greater_equal(1702.5, params['c'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cdf8477240ef3f3d86f186850dd3ba76",
          "grade": false,
          "grade_id": "cell-443acc848af1d47b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OXASR71-qG7G"
      },
      "source": [
        "### Problem 3: Making predictions with our fancier model\n",
        "\n",
        "Let's visualize the model we just trained using the following call to `plot_points_and_model`.  If you solved the previous two problems, you should see a nice curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "badeceab7d6f8965759201038d6e17f3",
          "grade": false,
          "grade_id": "cell-292bd0278e65e143",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4nV2lFDSqG7H"
      },
      "source": [
        "plot_points_and_model(rental_prices, 'x', y_hat, params, xlabel=\"Number of bedrooms\", ylabel=\"Monthly rent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "df7b45ca506505ffa689a9de5d7471b5",
          "grade": false,
          "grade_id": "cell-bb0216c115711a63",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "nzFrAQeZqG7L"
      },
      "source": [
        "For this problem, you will use the trained model `y_hat` to predict the rent for 4-bedroom rental units in Santa Cruz.  There are multiple ways to do this; here is what we suggest:\n",
        "\n",
        "  * Make a copy of the `params` dictionary.  (You can copy a dictionary using the `.copy()` method.)\n",
        "  * Add a new entry to the dictionary that associates `'x'` to a value.  For instance, if you want to predict the rent for a 2-bedroom rental unit, `'x'` should be associated with `2`.  The dictionary should now contain entries for all of the learned parameters, plus `'x'`.\n",
        "  * Call the `eval` method on `y_hat`, your model, passing your dictionary as the `env` argument.\n",
        "  \n",
        "You can do the above in no more than 3 lines of code.  If you're not sure what to do, the [lecture notebook](https://colab.research.google.com/drive/12gLy5eRVXxj0ur7p3ylnbc6o5izh_dye) has an example near the end.\n",
        "  \n",
        "The result of evaluating `y_hat` with an environment as described above will give you the predicted monthly rent of a unit with the specified number of bedrooms.\n",
        "\n",
        "**In the below cell, assign the predicted rent for 4-bedroom rental units to the variable `predicted_4_bedroom_rent`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40d79c0cd2df1ca504a2fbffa1bf0b2e",
          "grade": false,
          "grade_id": "cell-61fb92313a757df2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "VnpkDI0PqG7N"
      },
      "source": [
        "# In this cell, use your trained model, `y_hat`, to predict the rent for a 4-bedroom rental unit\n",
        "# and assign the resulting value to the variable `predicted_4_bedroom_rent`.\n",
        "\n",
        "env = params.copy()\n",
        "env['x'] = 4\n",
        "predicted_4_bedroom_rent=y_hat.eval(env=env)\n",
        "\n",
        "print(\"Predicted rent:\", predicted_4_bedroom_rent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d122754afb758a8672aee2d22b650d92",
          "grade": true,
          "grade_id": "cell-5ada75878800e515",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "X0QXe42GqG7R"
      },
      "source": [
        "assert_greater_equal(predicted_4_bedroom_rent, 5100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b020adc9b5497a0bde7eb6435eed5453",
          "grade": false,
          "grade_id": "cell-4ed0f0f815af6363",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "JmjEeT_gqG7W"
      },
      "source": [
        "Do you think the $\\hat{y} = ax^2 + bx + c$ model does a better job of predicting the price for 4-bedroom rentals than the $\\hat{y} = ax + b$ model that we saw in lecture did?  Feel free to compare the predictions with [real data](https://www.zillow.com/santa-cruz-ca/rentals/4-4_beds/)."
      ]
    }
  ]
}